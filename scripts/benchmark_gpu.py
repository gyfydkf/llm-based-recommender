#!/usr/bin/env python3
"""
GPU performance benchmark script for the recommendation system.
"""

import time
import json
import requests
import subprocess
import statistics
from datetime import datetime
from pathlib import Path

def get_gpu_info():
    """Get GPU information."""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=name,memory.total,driver_version", "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            line = result.stdout.strip().split('\n')[0]
            parts = line.split(', ')
            return {
                'name': parts[0],
                'memory_gb': int(parts[1]) // 1024,
                'driver_version': parts[2]
            }
        else:
            return None
    except Exception as e:
        print(f"è·å–GPUä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None

def benchmark_api_response():
    """Benchmark API response times."""
    print("ğŸ” æµ‹è¯•APIå“åº”æ—¶é—´...")
    
    test_queries = [
        "æ¨èä¸€äº›å¤å­£è¿è¡£è£™",
        "æˆ‘éœ€è¦ä¸€ä»¶é€‚åˆåŠå…¬å®¤çš„è¡¬è¡«",
        "æœ‰ä»€ä¹ˆé€‚åˆæ´¾å¯¹çš„è¡£æœå—",
        "æ¨èä¸€äº›èˆ’é€‚çš„ä¼‘é—²é‹",
        "æˆ‘éœ€è¦ä¸€ä»¶ä¿æš–çš„å¤–å¥—"
    ]
    
    response_times = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"  æµ‹è¯• {i}/{len(test_queries)}: {query}")
        
        try:
            start_time = time.time()
            response = requests.post(
                "http://localhost:8000/recommend",
                json={"question": query},
                timeout=60
            )
            end_time = time.time()
            
            if response.status_code == 200:
                response_time = end_time - start_time
                response_times.append(response_time)
                print(f"    âœ… å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            else:
                print(f"    âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except requests.exceptions.Timeout:
            print(f"    â° è¯·æ±‚è¶…æ—¶")
        except Exception as e:
            print(f"    âŒ è¯·æ±‚é”™è¯¯: {e}")
    
    return response_times

def benchmark_gpu_computation():
    """Benchmark GPU computation performance."""
    print("\nğŸ” æµ‹è¯•GPUè®¡ç®—æ€§èƒ½...")
    
    test_script = """
import torch
import time
import numpy as np

# Test GPU availability
if not torch.cuda.is_available():
    print("CUDAä¸å¯ç”¨")
    exit(1)

device = torch.device('cuda')
print(f"ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")

# Test matrix multiplication
sizes = [1000, 2000, 4000]
results = {}

for size in sizes:
    print(f"æµ‹è¯•çŸ©é˜µå¤§å°: {size}x{size}")
    
    # Create random matrices
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    
    # Warm up
    for _ in range(3):
        _ = torch.mm(a, b)
    
    torch.cuda.synchronize()
    
    # Benchmark
    start_time = time.time()
    for _ in range(10):
        c = torch.mm(a, b)
    torch.cuda.synchronize()
    end_time = time.time()
    
    avg_time = (end_time - start_time) / 10
    results[size] = avg_time
    print(f"  å¹³å‡æ—¶é—´: {avg_time:.4f}ç§’")

# Test memory bandwidth
print("\\næµ‹è¯•å†…å­˜å¸¦å®½...")
size = 5000
a = torch.randn(size, size, device=device)
b = torch.randn(size, size, device=device)

torch.cuda.synchronize()
start_time = time.time()
c = torch.mm(a, b)
torch.cuda.synchronize()
end_time = time.time()

memory_used = a.numel() * 4 * 2  # 2 matrices * 4 bytes per float
bandwidth = memory_used / (end_time - start_time) / 1e9  # GB/s
print(f"å†…å­˜å¸¦å®½: {bandwidth:.2f} GB/s")

print("\\næµ‹è¯•ç»“æœ:")
for size, time_taken in results.items():
    print(f"  {size}x{size}: {time_taken:.4f}ç§’")
"""
    
    try:
        result = subprocess.run(
            [sys.executable, "-c", test_script],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print("âœ… GPUè®¡ç®—æµ‹è¯•å®Œæˆ")
            print(result.stdout)
            return True
        else:
            print("âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥")
            print(result.stderr)
            return False
            
    except Exception as e:
        print(f"âŒ è¿è¡ŒGPUè®¡ç®—æµ‹è¯•æ—¶å‡ºé”™: {e}")
        return False

def benchmark_memory_usage():
    """Benchmark memory usage during inference."""
    print("\nğŸ” æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")
    
    try:
        # Get initial memory
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.used", "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            initial_memory = int(result.stdout.strip())
            print(f"åˆå§‹GPUå†…å­˜ä½¿ç”¨: {initial_memory} MB")
            
            # Simulate inference workload
            test_script = """
import torch
import time

if torch.cuda.is_available():
    # Load a model (simulate)
    model = torch.nn.Linear(1000, 1000).cuda()
    
    # Simulate inference
    for i in range(10):
        x = torch.randn(100, 1000).cuda()
        y = model(x)
        time.sleep(0.1)
    
    print("æ¨ç†æµ‹è¯•å®Œæˆ")
else:
    print("CUDAä¸å¯ç”¨")
"""
            
            subprocess.run([sys.executable, "-c", test_script], timeout=30)
            
            # Get final memory
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=memory.used", "--format=csv,noheader,nounits"],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                final_memory = int(result.stdout.strip())
                memory_increase = final_memory - initial_memory
                print(f"æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {final_memory} MB")
                print(f"å†…å­˜å¢åŠ : {memory_increase} MB")
                
                return {
                    'initial_memory': initial_memory,
                    'final_memory': final_memory,
                    'memory_increase': memory_increase
                }
        
        return None
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å†…å­˜ä½¿ç”¨æ—¶å‡ºé”™: {e}")
        return None

def generate_report(gpu_info, api_times, memory_usage):
    """Generate benchmark report."""
    print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'gpu_info': gpu_info,
        'api_performance': {
            'total_queries': len(api_times),
            'successful_queries': len([t for t in api_times if t > 0]),
            'average_response_time': statistics.mean(api_times) if api_times else 0,
            'min_response_time': min(api_times) if api_times else 0,
            'max_response_time': max(api_times) if api_times else 0,
            'response_times': api_times
        },
        'memory_usage': memory_usage,
        'performance_score': 0
    }
    
    # Calculate performance score
    if api_times:
        avg_time = statistics.mean(api_times)
        if avg_time < 3:
            report['performance_score'] = 5  # Excellent
        elif avg_time < 5:
            report['performance_score'] = 4  # Good
        elif avg_time < 8:
            report['performance_score'] = 3  # Average
        elif avg_time < 12:
            report['performance_score'] = 2  # Poor
        else:
            report['performance_score'] = 1  # Very Poor
    
    # Save report
    with open('gpu_benchmark_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    # Print summary
    print("\n" + "=" * 50)
    print("ğŸ“Š GPUæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    print("=" * 50)
    
    if gpu_info:
        print(f"ğŸ® GPU: {gpu_info['name']}")
        print(f"ğŸ’¾ å†…å­˜: {gpu_info['memory_gb']} GB")
        print(f"ğŸ”§ é©±åŠ¨: {gpu_info['driver_version']}")
    
    if api_times:
        print(f"\nğŸŒ APIæ€§èƒ½:")
        print(f"  æµ‹è¯•æŸ¥è¯¢æ•°: {len(api_times)}")
        print(f"  æˆåŠŸæŸ¥è¯¢æ•°: {len([t for t in api_times if t > 0])}")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {statistics.mean(api_times):.2f}ç§’")
        print(f"  æœ€å¿«å“åº”: {min(api_times):.2f}ç§’")
        print(f"  æœ€æ…¢å“åº”: {max(api_times):.2f}ç§’")
    
    if memory_usage:
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
        print(f"  åˆå§‹å†…å­˜: {memory_usage['initial_memory']} MB")
        print(f"  æœ€ç»ˆå†…å­˜: {memory_usage['final_memory']} MB")
        print(f"  å†…å­˜å¢åŠ : {memory_usage['memory_increase']} MB")
    
    # Performance rating
    ratings = {5: "ä¼˜ç§€", 4: "è‰¯å¥½", 3: "ä¸€èˆ¬", 2: "è¾ƒå·®", 1: "å¾ˆå·®"}
    score = report['performance_score']
    print(f"\nâ­ æ€§èƒ½è¯„çº§: {score}/5 ({ratings[score]})")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: gpu_benchmark_report.json")

def main():
    """Main benchmark function."""
    print("ğŸš€ GPUæ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # Check if API is running
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code != 200:
            print("âŒ APIæœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ç³»ç»Ÿ")
            return
    except:
        print("âŒ æ— æ³•è¿æ¥åˆ°APIæœåŠ¡ï¼Œè¯·å…ˆå¯åŠ¨ç³»ç»Ÿ")
        return
    
    # Get GPU info
    gpu_info = get_gpu_info()
    if not gpu_info:
        print("âŒ æ— æ³•è·å–GPUä¿¡æ¯")
        return
    
    print(f"ğŸ® æ£€æµ‹åˆ°GPU: {gpu_info['name']}")
    
    # Run benchmarks
    api_times = benchmark_api_response()
    benchmark_gpu_computation()
    memory_usage = benchmark_memory_usage()
    
    # Generate report
    generate_report(gpu_info, api_times, memory_usage)

if __name__ == "__main__":
    import sys
    main() 